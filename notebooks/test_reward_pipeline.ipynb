{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.MathTask'>\n",
      "<class '__main__.DateQuestionAnsweringTask'>\n",
      "<class '__main__.DebuggingTask'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'rouge', 'ngram': 'l', 'metric': 'f'},\n",
       " {'name': 'rouge', 'ngram': 'l', 'metric': 'f'},\n",
       " {'name': 'diff', 'lines': False, 'threshold': 0.5}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class DebuggingTask:\n",
    "    reward_models = [\n",
    "        dict(name='diff', lines=False, threshold=0.5),\n",
    "    ]\n",
    "    \n",
    "class SummarizationTask:\n",
    "    reward_models = [\n",
    "        dict(name='rouge', ngram='l', metric='f'),\n",
    "        dict(name='relevance', threshold=None),\n",
    "    ]    \n",
    "\n",
    "class QuestionAnsweringTask:\n",
    "    reward_models = [\n",
    "        dict(name='rouge', ngram='1', metric='f'),\n",
    "        dict(name='relevance', threshold=None),\n",
    "    ]    \n",
    "\n",
    "class MathTask:\n",
    "    reward_models = [\n",
    "        dict(name='rouge', ngram='l', metric='f'),\n",
    "    ]\n",
    "    \n",
    "class DateQuestionAnsweringTask:\n",
    "    reward_models = [\n",
    "        dict(name='rouge', ngram='l', metric='f'),\n",
    "    ]\n",
    "\n",
    "\n",
    "required_reward_models = []\n",
    "\n",
    "# selected tasks can come from wandb config, or other testing suites so that we can enable them as required\n",
    "selected_tasks = ['math', 'date_qa','debugging']\n",
    "\n",
    "# config can be set either \n",
    "all_tasks = {\n",
    "    'debugging': DebuggingTask, \n",
    "    'summarization': SummarizationTask, \n",
    "    'qa': QuestionAnsweringTask, \n",
    "    'math': MathTask, \n",
    "    'date_qa': DateQuestionAnsweringTask\n",
    "}\n",
    "\n",
    "for task in selected_tasks:\n",
    "    if task not in all_tasks:\n",
    "        raise ValueError(f'Task {task} not supported. Please choose from {all_tasks}')\n",
    "    \n",
    "    print(all_tasks[task])\n",
    "    required_reward_models += all_tasks[task].reward_models\n",
    "\n",
    "required_reward_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge': <__main__.RougeRewardModel at 0x7fd557247a60>,\n",
       " 'diff': <__main__.DiffRewardModel at 0x7fd557247d60>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import difflib\n",
    "from typing import List\n",
    "from angle_emb import AnglE\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from rouge import Rouge\n",
    "\n",
    "class RougeRewardModel:\n",
    "    \n",
    "    def __init__(self, ngram='l', metric='f', avg=False):\n",
    "        self.ngram = ngram\n",
    "        self.metric = metric\n",
    "        self.avg = avg\n",
    "        # TODO: Add init args to Rouge if required\n",
    "        self.rouge = Rouge()\n",
    "        \n",
    "    def rouge_score(self, reference, completion):\n",
    "        return self.rouge.get_scores(reference, completion, avg=self.avg)[0][self.metric][self.ngram]\n",
    "        \n",
    "    def reward(self, reference: str, completions: List[str]) -> torch.FloatTensor:\n",
    "        \"\"\"Compute ROUGE scores given a completion and reference pair.\"\"\"\n",
    "\n",
    "        return torch.FloatTensor([self.rouge_score(reference, completion) for completion in completions])\n",
    "\n",
    "\n",
    "class RelevanceRewardModel:\n",
    "    \n",
    "    def __init__(self, threshold=None, device='cuda'):\n",
    "        \n",
    "        self.threshold = threshold\n",
    "        self.model = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls')\n",
    "        if device == 'cuda':\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "    def reward(self, reference: str, completions: List[str]) -> torch.FloatTensor:\n",
    "\n",
    "        reference_embedding = self.model.encode(reference, to_numpy=False)\n",
    "        \n",
    "        completions_embeddings = self.model.encode(completions, to_numpy=False)\n",
    "        \n",
    "        return cosine_similarity(reference_embedding, completions_embeddings, dim=1)\n",
    "\n",
    "\n",
    "class DiffRewardModel:\n",
    "    \n",
    "    def __init__(self, lines=False, threshold=None):\n",
    "        self.lines = lines\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def unified_diff(self, reference, completion):\n",
    "        return len(difflib.unified_diff(reference.splitlines(), completion.splitlines()))\n",
    "    \n",
    "    def seq_match(self, reference, completion):\n",
    "        return difflib.SequenceMatcher(None, reference, completion).ratio()\n",
    "    \n",
    "    def reward(self, reference: str, completions: List[str]) -> torch.FloatTensor:\n",
    "        \"\"\"Get the score between two strings.\n",
    "        lines: If True, return a unified diff. If False, return a ratio.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.lines:\n",
    "            return torch.FloatTensor([self.unified_diff(reference, completion) for completion in completions])\n",
    "        else:\n",
    "            return torch.FloatTensor([self.seq_match(reference, completion) for completion in completions])\n",
    "\n",
    "\n",
    "all_reward_models = {\n",
    "    'rouge': RougeRewardModel,\n",
    "    'relevance': RelevanceRewardModel,\n",
    "    'diff': DiffRewardModel\n",
    "    \n",
    "}\n",
    "\n",
    "# Instantiate only the required reward models\n",
    "reward_models = {}\n",
    "for model in required_reward_models.copy():\n",
    "    name = model.pop('name')\n",
    "    reward_models[name] = all_reward_models[name](**model)\n",
    "\n",
    "\n",
    "reward_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
