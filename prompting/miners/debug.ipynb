{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 is attempting to access the resource.\n",
      "Thread 0 has accessed the resource.\n",
      "Thread 1 is attempting to access the resource.\n",
      "Thread 2 is attempting to access the resource.\n",
      "Thread 3 is attempting to access the resource.\n",
      "Thread 4 is attempting to access the resource.\n",
      "Thread 0 is releasing the resource.\n",
      "Thread 1 has accessed the resource.\n",
      "Thread 1 is releasing the resource.\n",
      "Thread 2 has accessed the resource.\n",
      "Thread 2 is releasing the resource.\n",
      "Thread 3 has accessed the resource.\n",
      "Thread 3 is releasing the resource.\n",
      "Thread 4 has accessed the resource.\n",
      "Thread 4 is releasing the resource.\n",
      "All threads have finished.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# Define a semaphore with 2 slots\n",
    "semaphore = threading.Semaphore(1)\n",
    "\n",
    "def access_resource(thread_number):\n",
    "    print(f\"Thread {thread_number} is attempting to access the resource.\")\n",
    "    # Acquire a slot\n",
    "    semaphore.acquire()\n",
    "    print(f\"Thread {thread_number} has accessed the resource.\")\n",
    "    # Simulate some resource-intensive work\n",
    "    time.sleep(1)\n",
    "    print(f\"Thread {thread_number} is releasing the resource.\")\n",
    "    # Release the slot\n",
    "    semaphore.release()\n",
    "\n",
    "# Create and start 5 threads\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    thread = threading.Thread(target=access_resource, args=(i,))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(\"All threads have finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/prompting/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 8/8 [03:24<00:00, 25.54s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:14<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> An increasing sequence: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/root/prompting/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one, three, five, seven, nine, etc.\n",
      "A decreasing sequence: ten, nine,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tok)\n",
    "\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> An increasing sequence: one, three, five, seven, nine, etc.\n",
      "A decreasing sequence: ten, nine,\n"
     ]
    }
   ],
   "source": [
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing with other operations while the background task runs.\n",
      "Finished other operations.\n",
      "Accessing the shared resource.\n",
      "Successfully accessed the shared resource within the timeout.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Shared resource and lock\n",
    "shared_resource = \"initial_value\"\n",
    "resource_lock = asyncio.Lock()\n",
    "\n",
    "async def access_resource():\n",
    "    \"\"\"A coroutine that simulates accessing and modifying the shared resource.\"\"\"\n",
    "    async with resource_lock:\n",
    "        # Simulate some work with the resource\n",
    "        print(\"Accessing the shared resource.\")\n",
    "        await asyncio.sleep(1)  # Simulate an operation that takes time\n",
    "\n",
    "async def attempt_access_with_timeout(timeout):\n",
    "    \"\"\"Attempts to access the shared resource with a specified timeout.\"\"\"\n",
    "    try:\n",
    "        await asyncio.wait_for(access_resource(), timeout)\n",
    "        print(\"Successfully accessed the shared resource within the timeout.\")\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Could not access the shared resource within {timeout} seconds.\")\n",
    "        # You might want to handle the exception here, depending on your application's needs\n",
    "\n",
    "async def main():\n",
    "    # Schedule attempt_access_with_timeout as a background task\n",
    "    background_task = asyncio.create_task(attempt_access_with_timeout(5))\n",
    "\n",
    "    # Continue with other steps immediately, without waiting for the above task to complete\n",
    "    print(\"Continuing with other operations while the background task runs.\")\n",
    "\n",
    "    # Example of other operations\n",
    "    #await asyncio.sleep(2)  # Simulate doing something else\n",
    "    print(\"Finished other operations.\")\n",
    "\n",
    "    # Optionally, wait for the background task to complete at some point\n",
    "    await background_task\n",
    "\n",
    "# Run the main coroutine\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/prompting/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id, \n",
    "            tokenizer=tokenizer,\n",
    "            device='cuda',\n",
    "            torch_dtype=torch.float16,            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pipeline call\n",
      "After pipeline call\n",
      "0 einstein's \n",
      "1 theory \n",
      "2 of \n",
      "3 \n",
      "4 \n",
      "5 relativity. \n",
      "6 I \n",
      "7 want \n",
      "8 to \n",
      "9 know \n",
      "10 everything \n",
      "11 about \n",
      "12 \n",
      "13 it, \n",
      "14 from \n",
      "15 the \n",
      "16 \n",
      "17 basics \n",
      "18 to \n",
      "19 the \n",
      "20 most \n",
      "21 complex \n",
      "22 \n",
      "23 concepts. \n",
      "24 Make \n",
      "25 it \n",
      "26 as \n",
      "27 detailed \n",
      "28 and \n",
      "29 technical \n",
      "30 as \n",
      "31 \n",
      "32 possible, \n",
      "33 but \n",
      "34 also \n",
      "35 explain \n",
      "36 it \n",
      "37 in \n",
      "38 a \n",
      "39 way \n",
      "40 that \n",
      "41 a \n",
      "42 \n",
      "43 layman \n",
      "44 can \n",
      "45 \n",
      "46 understand. \n",
      "47 Use \n",
      "48 clear \n",
      "49 and \n",
      "50 \n",
      "51 concise \n",
      "52 \n",
      "53 language, \n",
      "54 and \n",
      "55 provide \n",
      "56 examples \n",
      "57 and \n",
      "58 \n",
      "59 analogies \n",
      "60 where \n",
      "61 \n",
      "62 necessary. \n",
      "63 I \n",
      "64 want \n",
      "65 to \n",
      "66 be \n",
      "67 able \n",
      "68 to \n",
      "69 impress \n",
      "70 my \n",
      "71 friends \n",
      "72 with \n",
      "73 my \n",
      "74 \n",
      "75 newfound \n",
      "76 knowledge \n",
      "77 of \n",
      "78 \n",
      "79 \n",
      "80 \n",
      "81 relativity!</s>\n",
      "Finish streaming\n"
     ]
    }
   ],
   "source": [
    "from prompting.llm import CustomTextIteratorStreamer\n",
    "\n",
    "      \n",
    "streamer = CustomTextIteratorStreamer(tokenizer=tokenizer)\n",
    "\n",
    "print('Before pipeline call')\n",
    "_ = llm_pipeline('Write me a big long text about einstein', max_new_tokens=4096, streamer = streamer)\n",
    "print('After pipeline call')\n",
    "\n",
    "i = 0\n",
    "s = \"\"\n",
    "for t in streamer:\n",
    "    s += t\n",
    "    \n",
    "    if 'Write me a big long text about einsteTroubleshoin' not in s:\n",
    "        continue\n",
    "    \n",
    "    print(i, t)\n",
    "    i+= 1\n",
    "\n",
    "print('Finish streaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/prompting/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-21 03:08:48,329\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-21 03:08:49 llm_engine.py:87] Initializing an LLM engine with config: model='HuggingFaceH4/zephyr-7b-beta', tokenizer='HuggingFaceH4/zephyr-7b-beta', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-21 03:08:53 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 03-21 03:09:03 llm_engine.py:357] # GPU blocks: 5314, # CPU blocks: 2048\n",
      "INFO 03-21 03:09:05 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-21 03:09:05 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-21 03:09:10 model_runner.py:756] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "llm = LLM(model=model_id, gpu_memory_utilization=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LLM.generate() got an unexpected keyword argument 'stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTell me a fun history fact\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m composed_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<|system|>You are a helpful AI assistant\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m<|user|>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m<|assistant|>\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomposed_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m output\n",
      "\u001b[0;31mTypeError\u001b[0m: LLM.generate() got an unexpected keyword argument 'stream'"
     ]
    }
   ],
   "source": [
    "model_kwargs = dict()\n",
    "temperature = model_kwargs.get(\"temperature\", 0.8)\n",
    "top_p = model_kwargs.get(\"top_p\", 0.95)\n",
    "max_tokens = model_kwargs.get(\"max_tokens\", 256)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=temperature, top_p=top_p, max_tokens=max_tokens\n",
    ")\n",
    "\n",
    "prompt = 'Tell me a fun history fact'\n",
    "\n",
    "composed_prompt = f\"\"\"<|system|>You are a helpful AI assistant\n",
    "<|user|>{prompt} \n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "output = llm.generate(composed_prompt, sampling_params, use_tqdm=True, stream=True)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/prompting/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from prompting.llm import HuggingFaceLLM\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id, \n",
    "            tokenizer=tokenizer,\n",
    "            device='cuda',\n",
    "            torch_dtype=torch.float16,            \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMER\n",
      "<|system|>\n",
      "You are a helpful assistant</s> \n",
      "<|user|>\n",
      "Tell me about albert einstein</s> \n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "\n",
      "Albert \n",
      "Einstein \n",
      "was \n",
      "a \n",
      "\n",
      "renowned \n",
      "\n",
      "\n",
      "German-born \n",
      "\n",
      "\n",
      "physicist \n",
      "who \n",
      "made \n",
      "significant \n",
      "contributions \n",
      "to \n",
      "the \n",
      "field \n",
      "of \n",
      "\n",
      "science, \n",
      "most \n",
      "notably \n",
      "the \n",
      "development \n",
      "of \n",
      "the \n",
      "theory \n",
      "of \n",
      "\n",
      "\n",
      "relativity. \n",
      "Born \n",
      "in \n",
      "\n",
      "\n",
      "Ulm, \n",
      "\n",
      "Germany, \n",
      "in \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1879, \n",
      "Einstein \n",
      "showed \n",
      "an \n",
      "early \n",
      "\n",
      "aptitude \n",
      "for \n",
      "\n",
      "mathematics \n",
      "and \n",
      "\n",
      "physics. \n",
      "He \n",
      "went \n",
      "on \n",
      "to \n",
      "earn \n",
      "a \n",
      "degree \n",
      "in \n",
      "physics \n",
      "from \n",
      "the \n",
      "\n",
      "\n",
      "Polytechnic \n",
      "School \n",
      "in \n",
      "\n",
      "Zurich \n",
      "and \n",
      "later \n",
      "a \n",
      "\n",
      "\n",
      "\n",
      "Ph.D. \n",
      "From \n",
      "the \n",
      "University \n",
      "of \n",
      "\n",
      "\n",
      "Zurich.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Einstein's \n",
      "\n",
      "groundbreaking \n",
      "work \n",
      "in \n",
      "physics \n",
      "began \n",
      "in \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1905, \n",
      "when \n",
      "he \n",
      "published \n",
      "four \n",
      "papers \n",
      "that \n",
      "would \n",
      "come \n",
      "to \n",
      "be \n",
      "known \n",
      "as \n",
      "his \n",
      "\n",
      "\n",
      "\"annus\n"
     ]
    }
   ],
   "source": [
    "streamer = HuggingFaceLLM(llm_pipeline, 'You are a helpful assistant', max_new_tokens=128).stream('Tell me about albert einstein')\n",
    "\n",
    "print('STREAMER')\n",
    "for t in streamer:\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
