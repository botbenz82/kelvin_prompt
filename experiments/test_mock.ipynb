{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bittensor as bt\n",
    "\n",
    "from prompting.llm import HuggingFaceLLM\n",
    "# from prompting.agent import HumanAgent\n",
    "# from prompting.tasks import SummarizationTask\n",
    "# from prompting.tools import WikiDataset\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import torch\n",
    "from transformers import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MockTokenizer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MockTokenizer, self).__init__()\n",
    "\n",
    "        self.role_expr = '<|mock-{role}|>'\n",
    "\n",
    "    def apply_chat_template(self, messages, **kwargs):\n",
    "        prompt = ''\n",
    "        for m in messages:\n",
    "            role = self.role_expr.format(role=m['role'])\n",
    "            content = m['content']\n",
    "            prompt += f'<|mock-{role}|> {content}\\n'\n",
    "\n",
    "        return '\\n'.join(prompt)\n",
    "\n",
    "\n",
    "class MockModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, phrase):\n",
    "\n",
    "        super(MockModel, self).__init__()\n",
    "\n",
    "        self.tokenizer = MockTokenizer()\n",
    "        self.phrase = phrase\n",
    "\n",
    "    def __call__(self, messages):\n",
    "        return self.forward(messages)\n",
    "\n",
    "    def forward(self, messages):\n",
    "        role_tag = self.tokenizer.role_expr.format(role='assistant')\n",
    "        return f'{role_tag} {self.phrase}'\n",
    "\n",
    "\n",
    "\n",
    "class MockPipeline():\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self.model.tokenizer\n",
    "\n",
    "    def __init__(self, model_id='mock', device_map='mock-cuda', torch_dtype='torch.mock16', phrase='mock reply'):\n",
    "\n",
    "        super(MockPipeline, self).__init__()\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.device_map = device_map\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.model = MockModel(phrase)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(model_id={self.model_id}, device_map={self.device_map}, torch_dtype={self.torch_dtype})'\n",
    "\n",
    "    def __call__(self, messages, **kwargs):\n",
    "        return self.forward(messages, **kwargs)\n",
    "\n",
    "    def forward(self, messages, **kwargs):\n",
    "\n",
    "        output = self.model(messages)\n",
    "        return self.postprocess(output)\n",
    "\n",
    "    def postprocess(self, output, **kwargs):\n",
    "        return [{'generated_text':output}]\n",
    "\n",
    "    def preprocess(self, **kwargs):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bt.logging.info(\"ðŸ¤– Loading LLM model...\")\n",
    "\n",
    "# llm_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     #device_map=\"cuda:0\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# bt.logging.info(\"Creating task...\")\n",
    "# dataset = WikiDataset()\n",
    "# context = dataset.next()\n",
    "\n",
    "# task = SummarizationTask(llm_pipeline=llm_pipeline, context=context)\n",
    "\n",
    "# bt.logging.info(\"Creating agent...\")\n",
    "# agent = HumanAgent(\n",
    "#     task = task,\n",
    "#     llm=llm_pipeline,\n",
    "#     system_template=None,\n",
    "#     begin_conversation=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = MockPipeline(phrase='wut you say??')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = HuggingFaceLLM(pipeline=pipeline, system_prompt='u a badas')\n",
    "agent.query('wassup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor as bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockMetagraph(bt.metagraph):\n",
    "\n",
    "    def __init__(self, netuid=1, network='mock', sync=False):\n",
    "        super(MockMetagraph, self).__init__(netuid=netuid, network=network, sync=sync)\n",
    "\n",
    "        subtensor = bt.MockSubtensor()\n",
    "        if not subtensor.subnet_exists(netuid):\n",
    "            subtensor.create_subnet(netuid)\n",
    "\n",
    "        for i in range(10):\n",
    "            subtensor.force_register_neuron(netuid=netuid, hotkey=str(i), coldkey=str(i), balance=100000, stake=100000)\n",
    "\n",
    "        self.sync(subtensor=subtensor)\n",
    "\n",
    "\n",
    "m = MockMetagraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AxonInfo( /ipv0/0.0.0.0:0, 0, 0, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 1, 1, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 2, 2, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 3, 3, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 4, 4, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 5, 5, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 6, 6, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 7, 7, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 8, 8, 0 ),\n",
       " AxonInfo( /ipv0/0.0.0.0:0, 9, 9, 0 )]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.axons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.hotkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.uids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.last_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
