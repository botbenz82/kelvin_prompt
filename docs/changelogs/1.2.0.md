# Release Notes for prompting Version 1.2.0
## Date
[Release Date]

## What's Changed

TODO: Add release notes

## TL;DR

### Results

Practically speaking, the adoption of vllm results in an efficiency gain of 8.85%-14.17% in low tier devices (A10, A40, A6000) and 54.88%-57.33% in medium tier devices (A100), meaning that our validators can be twice as fast with vllm, increasing the generation  throughput of the network.

![Main comparison](imgs/1.2.0_plots_no_gpu_rest.png)


## Known Issues and Solutions

Although [vllm](https://github.com/vllm-project/vllm) is a popular framework, the project is barely 1 year old with 859 issues open in their github at the moment of this writing (2024-03-14), showing that the open source project is yet on its top of maturity, growing as we speak.

Alongside the complications derived directly from limitations of the vllm project, we also face complications regarding dependency conflicts between vllm and bittensor/prompting, as detailed below. So as overall suggestion, **we strongly recommend everyone to create a new python environment to install this new version**, as we are dealing with an workaround solution for the current conjucture we are. So in case you see an unexpected behaviour on your validator after this update, **please, try to recreate your environment first before trying troubleshoot with the known inssues below or before engaging in discord asking for help**


- **Issue 1 - Dependency conflict**: 
    Currently we have a set of of dependencies that conflict with each other, more specifically:
    ```bash
    The conflict is caused by:
        prompting 1.1.2 depends on transformers==4.36.2 vllm 0.3.0 depends on transformers>=4.37.0
        bittensor 6.6.0 depends on pydantic!=1.8, !=1.8.1, <2.0.0 and >=1.7.4 vllm 0.3.0 depends on pydantic>=2.0
    ```

    The AnglE Embedding model we use on our reward stack is dependent of a `transformers==4.36.2`, which conflicts with vllm's `transformers>=4.37.0`. This conflict doesn't affect directly vllm for the purposes we are using, so we should be fine.

    **Solution**: 
    - Recreate your environment following the README instructions
    - run the following commands on your python environment:
    ```python
    pip install pydantic==1.10.7 transformers==4.36.2
    ```


- **Issue 2 - RuntimeError: There is no current event loop in thread**:     
    vllm install in its requirements [uvloop](https://github.com/MagicStack/uvloop), which conflicts directly with bittensor.    
    **Solution**:
    - Recreate your environment following the README instructions
    - run the following command on your python environment:
    ```python    
    pip uninstall uvloop
    ```

- **Issue 3 - Specifying GPU device to be used with vllm**:

    Due to a current limitation of vllm, itâ€™s currently necessary to set your `CUDA_VISIBLE_DEVICES` environment variable manually in order to specify a gpu device to vllm:
    ```
    export CUDA_VISIBLE_DEVICES=1,2
    python neurons/validator.py ...
    ```
    
    Current open issues and discussions regarding this:
    - [How to specify which gpu to use?Â vllm-project/vllm#691](https://github.com/vllm-project/vllm/discussions/691)
    - [Unable to specify GPU usage in VLLM codeÂ vllm-project/vllm#3012](https://github.com/vllm-project/vllm/issues/3012)
    - [How to specify which GPU the model inference on?Â vllm-project/vllm#352](https://github.com/vllm-project/vllm/issues/352)



## Experiment:
Given the latest implementation of vLLM pipeline on https://github.com/opentensor/prompting/tree/features/vllm-test, perform a benchmark on how vLLM and Hugging Face pipelines compare in timing in different machines.

**Dataset:**
100 random samples of qa challenges collected from wandb to be answered by each LLM with their respective pipelines.
Machines:
- Runpod A40
- Runpod A6000
- Runpod A100 SXM4 80GB
- Lambda A10
- Lambda A100 SXM4 40GB

The tests were collected replicating the hugging face pipeline used in the validator code so far and the proposed vllm validator pipeline. 
Below the gpu footprint for each model:

| Machine | total_available_memory | zephyr vllm | zephyr vllm validator pipeline with 24GB gpu limitation | zephyr ðŸ¤— hf | zephyr ðŸ¤— hf validator pipeline  |
| --- | --- | --- | --- | --- | --- |
| Lambda A10 | 23GB | 16.25GB | 15.35GB | OOM | 15GB |
| Lambda A100 | 40.9GB | 34.58GB | 22.31GB | 31.35GB | 15.9GB |
| RunPod A40 | 46GB | 36.89 | 14.99GB | 31.4GB | 15.3GB |
| RunPod A100 | 81.92GB | 69GB | 15.45GB | 31GB | 15.3GB |
| RunPod A6000 | 49.14GB | 39.74GB | 15.41GB | 39.78GB | 15.8GB |

The main difference between  **zephyr ðŸ¤— hf** and **zephyr ðŸ¤— hf validator pipeline**  is that the pipeline is the replication of the validator code, which leverages `torch_dtype_float16` to optimize the model inference and gpu footprint. Itâ€™s worth mentioning that ðŸ¤— zephyr doesnâ€™t load on an A10 without torch_dtype_float16 due to OOM exceptions.


## Results

The plots below illustrate the different distribution on time for 2 different scenarios, 
- **No GPU / torch restrictions**: The model is loaded with minimum setup, meaning no `gpu_utilization` on vllm and no use of `torch_dtype_float16` on ðŸ¤— hf.
- **With GPU / torch restrictions:** Implemented pipeline code that limits `gpu_utilization` on vllm and leverages `torch_dtype_float16` on ðŸ¤— hf (current validator implementation).

**No GPU / torch restrictions:**
![No_GPU_restriction_plot](imgs/1.2.0_plots_no_gpu_rest.png)

| Machine | vllm_avg_in_secs | hf_avg_in_secs | % vllm_efficiency_gain |
| --- | --- | --- | --- |
| Lambda A10 | 8.41 | NaN | NaN |
| Lambda A100 | 3.29 | 6.94 | 52.59% |
| RunPod A100 | 2.95 | 6.67 | 55.77% |
| RunPod A40 | 6.75 | 13.05 | 48.28% |
| RunPod A6000 | 9.73 | 14.53 | 33.04% |

**With GPU / torch restrictions:**
![GPU_restriction_plot](imgs/1.2.0_plots_gpu_rest.png)

| machine | vllm_with_restrictions_avg_in_secs | hf_with_restrictions_avg_in_secs | % vllm_efficiency_gain |
| --- | --- | --- | --- |
| Lambda A10 | 8.34 | 9.15 | 8.85% |
| Lambda A100 | 3.28 | 7.27 | 54.88% |
| RunPod A100 | 2.94 | 6.89 | 57.33% |
| RunPod A40 | 6.75 | 7.83 | 13.79% |
| RunPod A6000 | 9.75 | 11.36 | 14.17% |

We can observe that the performance of vllm doesnâ€™t change much with our without gpu restrictions, which differs from hf where the time performance seems to improve on most of scenarios (A40, A100s, A6000)

Speaking production wise, where vllm is set with gpu restrictions targeting max usage of 20 and 24GB, the vllm efficiency gain varied between 8% and 57.33%, where:

- Small GPU and slow inference (A10): Efficiency gain of 8.85%
- Medium GPU and slow inference (A40, A6000): Efficiency gain of 13.79-14.17%
- Medium/Big GPUs and fast inference (A100s): Efficiency gain of 54.88-57.33%


